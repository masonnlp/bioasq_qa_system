{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BioASQ_QA_System.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c3ee96b7add54b65bf2615737992608a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e4c14cc2e9eb4e7dab17e2c20de44e94",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5c916a5b37bb4d9b90e011579d7797cc",
              "IPY_MODEL_d34b70c8ce2e47b181df932be3a1110b"
            ]
          }
        },
        "e4c14cc2e9eb4e7dab17e2c20de44e94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c916a5b37bb4d9b90e011579d7797cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d69c6e3656c24bf3aa4bd482c6ddb97c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_165330e665e0445986dd22d6c1b2d980"
          }
        },
        "d34b70c8ce2e47b181df932be3a1110b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_75d8873eb245476b905a968f8b25d9f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 614kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3945d0de9f5b476eb59f480b76c0e748"
          }
        },
        "d69c6e3656c24bf3aa4bd482c6ddb97c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "165330e665e0445986dd22d6c1b2d980": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75d8873eb245476b905a968f8b25d9f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3945d0de9f5b476eb59f480b76c0e748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masonnlp/bioasq_qa_system/blob/master/BioASQ_QA_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Z4c1stH0Jy",
        "outputId": "995939fa-9b05-4f42-afb8-707a6d82434d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga-BTwK-ICcj",
        "outputId": "f48327b3-7d9f-48d0-92ef-2bbc36d7741d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install transformers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device=torch.device('cuda')\n",
        "from transformers import BertTokenizer,BertForSequenceClassification,AdamW,BertConfig,get_linear_schedule_with_warmup\n",
        "from lxml import etree as ET\n",
        "!pip3 install scispacy\n",
        "!pip3 install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
        "import spacy\n",
        "import scispacy\n",
        "import en_core_sci_lg\n",
        "nlp = en_core_sci_lg.load()\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 30.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 51.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 50.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=87049ef7e4c1f8108c74c11b8ec7b5f115c44a30861277dd383c03e3979e7ab7\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n",
            "Collecting scispacy\n",
            "  Downloading https://files.pythonhosted.org/packages/93/0d/db88d1c2ad059c0b6854ca00519038b36ee1106e97899a3eb66bd25aeaaa/scispacy-0.2.5-py3-none-any.whl\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from scispacy) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0conllu,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scispacy) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from scispacy) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scispacy) (1.18.5)\n",
            "Collecting pysbd\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/db/95bd39a94eae9a5149bfde3d27760fb3595a35e11a9a01f6e97288132475/pysbd-0.3.3-py3-none-any.whl (67kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 5.9MB/s \n",
            "\u001b[?25hCollecting spacy<3.0.0,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/b5/c7a92c7ce5d4b353b70b4b5b4385687206c8b230ddfe08746ab0fd310a3a/spacy-2.3.2-cp36-cp36m-manylinux1_x86_64.whl (9.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.0MB 9.2MB/s \n",
            "\u001b[?25hCollecting nmslib>=1.7.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/fd/7d7428d29f12be5d1cc6d586d425b795cc9c596ae669593fd4f388602010/nmslib-2.0.6-cp36-cp36m-manylinux2010_x86_64.whl (12.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.0MB 242kB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0conllu,>=2.0.0->scispacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0conllu,>=2.0.0->scispacy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0conllu,>=2.0.0->scispacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0conllu,>=2.0.0->scispacy) (2.10)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.3->scispacy) (1.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (50.3.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (0.4.1)\n",
            "Collecting thinc==7.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/ae/ef3ae5e93639c0ef8e3eb32e3c18341e511b3c515fcfc603f4b808087651/thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1MB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (3.0.2)\n",
            "Collecting pybind11>=2.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/e3/d576f6f02bc75bacbc3d42494e8f1d063c95617d86648dba243c2cb3963e/pybind11-2.5.0-py2.py3-none-any.whl (296kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296kB 44.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from nmslib>=1.7.3.6->scispacy) (5.4.8)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.0->scispacy) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.0->scispacy) (3.2.0)\n",
            "Installing collected packages: pysbd, thinc, spacy, pybind11, nmslib, scispacy\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed nmslib-2.0.6 pybind11-2.5.0 pysbd-0.3.3 scispacy-0.2.5 spacy-2.3.2 thinc-7.4.1\n",
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
            "\u001b[?25l  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz (500.6MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500.6MB 33kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.1 in /usr/local/lib/python3.6/dist-packages (from en-core-sci-lg==0.2.4) (2.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (50.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (0.8.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.18.5)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (7.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (2020.6.20)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-core-sci-lg==0.2.4) (3.2.0)\n",
            "Building wheels for collected packages: en-core-sci-lg\n",
            "  Building wheel for en-core-sci-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-sci-lg: filename=en_core_sci_lg-0.2.4-cp36-none-any.whl size=501343162 sha256=aa334d61093df11e50c7a0e6b16de02c71da751a6b76cee3f03b58a909bae05f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/ab/e5/fa667519032799529ce6a50944a82d6ae3603819cd07836aa2\n",
            "Successfully built en-core-sci-lg\n",
            "Installing collected packages: en-core-sci-lg\n",
            "Successfully installed en-core-sci-lg-0.2.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_sci_lg' (0.2.4) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ued1Px0fZhF1"
      },
      "source": [
        "Read input file (.csv) and predict type for each question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5FspDEzLT1i",
        "outputId": "ac873544-2d46-4457-ca6d-7f775c7b9c5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540,
          "referenced_widgets": [
            "c3ee96b7add54b65bf2615737992608a",
            "e4c14cc2e9eb4e7dab17e2c20de44e94",
            "5c916a5b37bb4d9b90e011579d7797cc",
            "d34b70c8ce2e47b181df932be3a1110b",
            "d69c6e3656c24bf3aa4bd482c6ddb97c",
            "165330e665e0445986dd22d6c1b2d980",
            "75d8873eb245476b905a968f8b25d9f3",
            "3945d0de9f5b476eb59f480b76c0e748"
          ]
        }
      },
      "source": [
        "def preprocess(df):\n",
        "  df.encoded_tokens = [tokenizer.encode_plus(text,add_special_tokens=True)['input_ids'] for text in df['Question']] #encoded tokens for each tweet\n",
        "  df.attention_mask = [tokenizer.encode_plus(text,add_special_tokens=True)['attention_mask'] for text in df['Question']]\n",
        "  encoded_tokens = list(df.encoded_tokens)\n",
        "  attention_mask = list(df.attention_mask)\n",
        "  return encoded_tokens,attention_mask\n",
        "\n",
        "# Convert indices to Torch tensor and dump into cuda\n",
        "def feed_generator(encoded_tokens,attention_mask):\n",
        "\n",
        "    batch_size = 16\n",
        "    batch_seq = [x for x in range(int(len(encoded_tokens)/batch_size))]\n",
        "\n",
        "\n",
        "    shuffled_encoded_tokens,shuffled_attention_mask = encoded_tokens,attention_mask\n",
        "\n",
        "    res = len(encoded_tokens)%batch_size\n",
        "    if res != 0:\n",
        "        batch_seq = [x for x in range(int(len(encoded_tokens)/batch_size)+1)]\n",
        "    shuffled_encoded_tokens = shuffled_encoded_tokens+shuffled_encoded_tokens[:res]\n",
        "    shuffled_attention_mask = shuffled_attention_mask+shuffled_attention_mask[:res]\n",
        "\n",
        "    for batch in batch_seq:\n",
        "        maxlen_sent = max([len(i) for i in shuffled_encoded_tokens[batch*batch_size:(batch+1)*batch_size]])\n",
        "        token_tensor = torch.tensor([tokens+[0]*(maxlen_sent-len(tokens)) for tokens in shuffled_encoded_tokens[batch*batch_size:(batch+1)*batch_size]])\n",
        "        attention_mask = torch.tensor([tokens+[0]*(maxlen_sent-len(tokens)) for tokens in shuffled_attention_mask[batch*batch_size:(batch+1)*batch_size]]) \n",
        "\n",
        "        token_tensor = token_tensor.to('cuda')\n",
        "        attention_mask = attention_mask.to('cuda')\n",
        "\n",
        "        yield token_tensor,attention_mask\n",
        "\n",
        "def predict(model,data):\n",
        "    model.eval()\n",
        "    model.cuda()\n",
        "    preds = []\n",
        "    batch_count = 0\n",
        "    for token_tensor, attention_mask in data:\n",
        "        with torch.no_grad():\n",
        "            logits = model(token_tensor,token_type_ids=None,attention_mask=attention_mask)[0]\n",
        "        tmp_preds = torch.argmax(logits,-1).detach().cpu().numpy().tolist()\n",
        "        preds += tmp_preds             \n",
        "    return preds\n",
        "\n",
        "test_data_path = '/content/gdrive/My Drive/Colab Notebooks/BioASQ/input.csv'\n",
        "testing_df = pd.read_csv(test_data_path,sep=',',header=0)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#checkpoint_dir = \"gdrive/My Drive/Colab Notebooks/bert-large-v3/\"\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('/content/gdrive/My Drive/Colab Notebooks/BioASQ/Model/', cache_dir=None)\n",
        "\n",
        "encoded_tokens_Test,attention_mask_Test = preprocess(testing_df)\n",
        "data_test = feed_generator(encoded_tokens_Test, attention_mask_Test)\n",
        "preds_test = predict(model,data_test)\n",
        "\n",
        "\n",
        "indices_to_label = {0: 'factoid', 1: 'list', 2: 'summary', 3: 'yesno'}\n",
        "\n",
        "predict_label = []\n",
        "for i in preds_test[0:len(testing_df['Question'])]:\n",
        "  for j in indices_to_label:\n",
        "    if i == j:\n",
        "      predict_label.append(indices_to_label[j])\n",
        "\n",
        "testing_df['type'] = predict_label\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3ee96b7add54b65bf2615737992608a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mbyte\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-450964837f63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Colab Notebooks/BioASQ/Model/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mencoded_tokens_Test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask_Test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 raise OSError(\n\u001b[0;32m--> 930\u001b[0;31m                     \u001b[0;34m\"Unable to load weights from pytorch checkpoint file. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m                     \u001b[0;34m\"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 )\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNqZoyumZs3R"
      },
      "source": [
        "Create output file (XML) providing question type to Answer Processing system and query for Information Retrieval system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR1mRNENGMWH"
      },
      "source": [
        "def xml_tree(df):\n",
        "    root = ET.Element(\"Input\")\n",
        "    for ind in df.index:\n",
        "      id = df['ID'][ind]\n",
        "      question = df['Question'][ind]\n",
        "      qtype = df['type'][ind]\n",
        "      q = ET.SubElement(root,\"Q\")\n",
        "      q.set('id',str(id))\n",
        "      q.text = question\n",
        "      qp = ET.SubElement(q,\"QP\")\n",
        "      qp_type = ET.SubElement(qp,'Type')\n",
        "      qp_type.text = qtype\n",
        "      doc = nlp(question)\n",
        "      ent_list = []\n",
        "      for ent in doc.ents:\n",
        "        ent_list.append(str(ent))\n",
        "        qp_en = ET.SubElement(qp,'Entities') \n",
        "        qp_en.text = str(ent)\n",
        "      qp_query = ET.SubElement(qp,'Query')\n",
        "      qp_query.text = str(' '.join(ent_list))\n",
        "      # Create IR tag\n",
        "      IR = ET.SubElement(q, \"IR\")\n",
        "      \n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write('gdrive/My Drive/Colab Notebooks/BioASQ/qp_demo.xml', pretty_print=True)\n",
        "    \n",
        "\n",
        "xml_tree(testing_df)    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QDvrSeKiuK0"
      },
      "source": [
        "**Start IR Module**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ay1g3DAV_eI"
      },
      "source": [
        "PubmedArticle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ5I_fgxWDvX"
      },
      "source": [
        "\"\"\"\n",
        "This module implements the class DataSetReader which contains\n",
        " the implementation of code to read the BioAsq dataset\n",
        "\"\"\"\n",
        "from typing import List\n",
        "\n",
        "class PubmedArticle:\n",
        "\n",
        "    def fromDict(data: dict):\n",
        "        pmid = data[\"pmid\"]\n",
        "        title = data[\"title\"]\n",
        "        journal = data[\"journal\"]\n",
        "        mesh_major = data[\"meshMajor\"]\n",
        "        year = data[\"year\"]\n",
        "        abstract_text = data[\"abstractText\"]\n",
        "        return PubmedArticle(pmid, title, journal,\n",
        "                             year, abstract_text, mesh_major)\n",
        "\n",
        "    def __init__(self, pmid: str, title: str, journal: str,\n",
        "                 year: str, abstract_text: str, mesh_major: List[str]):\n",
        "        self.journal = journal\n",
        "        self.mesh_major = mesh_major\n",
        "        self.year = year\n",
        "        self.abstract_text = abstract_text\n",
        "        self.pmid = pmid\n",
        "        self.title = title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od1-NjWrWO0i"
      },
      "source": [
        "PubmedReader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUw6z2-dWTT1"
      },
      "source": [
        "\"\"\"\n",
        "This modeule implements reading pubmed xml fragments\n",
        "\"\"\"\n",
        "import os\n",
        "import gzip\n",
        "import xml.etree.ElementTree as ET\n",
        "from typing import List\n",
        "# No need to import PubmedArticle since it's in the same notebook\n",
        "\n",
        "\n",
        "class PubmedReader:\n",
        "    \"\"\"\n",
        "    This class is responsible for reading the Pubmed dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        default constructor doesn't do anything\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def get_xml_frags(self, dir: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        given a directory where all the xml fragments reside\n",
        "        will return the list of all the xml fragments\n",
        "        \"\"\"\n",
        "        file_names = os.listdir(dir)\n",
        "        file_indexes = [i for i, val in enumerate(\n",
        "            map(lambda nm: nm.startswith(\"pubmed\")\n",
        "                and nm.endswith(\".xml.gz\"),\n",
        "                file_names)) if val]\n",
        "        return list(map(lambda i: file_names[i], file_indexes))\n",
        "\n",
        "    def process_xml_frags(\n",
        "            self, dir: str,\n",
        "            max_article_count: int):\n",
        "        frags = self.get_xml_frags(dir)\n",
        "        remaining_count = max_article_count\n",
        "        for frag in frags:\n",
        "            if remaining_count > 0:\n",
        "                articles = self.process_xml_frag(dir + \"/\"\n",
        "                                                 + frag, remaining_count)\n",
        "                remaining_count -= len(articles)\n",
        "                if len(articles) == 0:\n",
        "                    break\n",
        "                for article in articles:\n",
        "                    yield article\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    def process_xml_frag(\n",
        "            self, fname: str, max_article_count:\n",
        "            int):\n",
        "        \"\"\"\n",
        "        This method reads to a complete gzipped xml file\n",
        "        and extracts each PubmedArticle, and returns a list\n",
        "        of PubmedArticle objects that contain all the relevant\n",
        "        fields\n",
        "        \"\"\"\n",
        "        articles = []\n",
        "        with gzip.open(fname, 'rt', encoding=\"utf-8\") as f:\n",
        "            count = 0\n",
        "            pubmed_article_txt = \"\"\n",
        "            record = False\n",
        "            while True:\n",
        "                line = f.readline()\n",
        "                if not line:\n",
        "                    break\n",
        "                if '<PubmedArticle>' in line:\n",
        "                    record = True\n",
        "                if record:\n",
        "                    pubmed_article_txt += line\n",
        "                if '</PubmedArticle>' in line:\n",
        "                    if count >= max_article_count:\n",
        "                        print(\"reached max article count ending read\")\n",
        "                        break\n",
        "                    count += 1\n",
        "                    record = False\n",
        "                    articles.append(\n",
        "                        self.process_pubmed_article_xml(pubmed_article_txt))\n",
        "                    pubmed_article_txt = \"\"\n",
        "        print(\"fname\", fname, \"articles\", count)\n",
        "        return articles\n",
        "\n",
        "    def process_pubmed_article_xml(self, txt: str) -> PubmedArticle:\n",
        "        \"\"\"\n",
        "        this article takes an XML fragment of a single Pubmed article\n",
        "        entry and parses it for data\n",
        "        It returns a populated PubmedArticle object\n",
        "        \"\"\"\n",
        "        root = ET.fromstring(txt)\n",
        "        pmid = root.findtext('.//PMID')\n",
        "        title = root.findtext('.//ArticleTitle')\n",
        "        abstract_text = root.findtext('.//AbstractText')\n",
        "        journal = root.findtext('.//Title')\n",
        "        if root.findtext('.//PubDate/Year'):\n",
        "          year = root.findtext('.//PubDate/Year')\n",
        "        else:\n",
        "          year = 0000\n",
        "        mesh_major = list(\n",
        "            map(lambda x: x.text, root.findall(\".//DescriptorName\")))\n",
        "        return PubmedArticle(\n",
        "            pmid, title, journal, year, abstract_text, mesh_major)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVtd25juWzG8"
      },
      "source": [
        "PubmedIndexer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTsSBSqeXViN"
      },
      "source": [
        "Install Whoosh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6WYS7GBXUEs"
      },
      "source": [
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTXSjCpsW3oy"
      },
      "source": [
        "\"\"\"\n",
        "This module indexes the Pubmed dataset using Whoosh\n",
        "\"\"\"\n",
        "import os\n",
        "import os.path\n",
        "import shutil\n",
        "from whoosh import index\n",
        "from whoosh.fields import Schema, TEXT, IDLIST, ID, NUMERIC\n",
        "from whoosh.analysis import StemmingAnalyzer\n",
        "from whoosh.qparser import QueryParser\n",
        "from datetime import datetime\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class PubmedIndexer:\n",
        "    \"\"\"\n",
        "    PubmedIndexer is the main class that clients are expected to to use.\n",
        "    The primary functions it performs are:\n",
        "    1. Indexing the pubmed articles into a Whoosh index\n",
        "    2. Allowing the free text searching of the pubmed articles\n",
        "\n",
        "    NOTES:\n",
        "    1. The pubmed data is provided here:\n",
        "      ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/\n",
        "    2. We do not index all the fields per article -- we index:\n",
        "      a. The pubmed ID\n",
        "      b. The Journal name\n",
        "      c. The Year of publication\n",
        "      d. The Article title\n",
        "      e. The Article Abstract\n",
        "    3. The complete pubmed dataset is just under 7 GB of compressed\n",
        "      XML shards (as of this writing)\n",
        "    4. This module allows all this data to be indexed\n",
        "    5. The index takes about 5 hours to generate on a medium powered laptop\n",
        "    6. The index directly is roughly 7 GB\n",
        "    7. The index directory can be tarred(zipped) and shared between users\n",
        "    8. We will probably rename this module pubmed_ir soon and relase it to PyPI\n",
        "\n",
        "    MISSING & DESIRABLE FUNCTIONALITY\n",
        "    1. It would be good to have utility function that is able to download\n",
        "      the pub med data\n",
        "    2. We should get __init__.py, etc. files done so we can publish to PyPi\n",
        "    3. We should have a partial indexing feature that indexes only data needed\n",
        "       for biosqr task b\n",
        "    4. We might make the index generation system more customization interms\n",
        "       of things such as Analyzers, stop-words, etc.\n",
        "    5. We may need a customizable result scoring function -- beyond BM25\n",
        "    6. We may want a more sophisticated querying interface, boolean queries, etc\n",
        "    7. We need a lot of testing to certify the system\n",
        "    8. It is not clear if we can add documents to an existing index\n",
        "    9. It is not clear how we can re-index an existing index\n",
        "    10. We should swap out prints with a formal logging framework\n",
        "    11. We should have example modules which demonstrate the use of this system\n",
        "    12. We really need to modify the directory structure of the project\n",
        "\n",
        "    BUGS & KNOWN LIMITATIONS\n",
        "    1. At the moment the free text query only searches the Abstract Text\n",
        "      it does not search the title\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        default construstor it does nothing at the moment\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def mk_index(self, indexpath: str = \"indexdir\",\n",
        "                 overwrite: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        creates a Whoosh based index for subsequent IR operatons\n",
        "\n",
        "        Prameters\n",
        "        ---------\n",
        "        indexpath: str\n",
        "            The absolute or relative path where you want the index to be stored\n",
        "               Note: the index path is a directory\n",
        "               this directory will contain all the Whoosh files\n",
        "        overwrite: boolean\n",
        "            This will overwrite any existing index (directory) if set to True\n",
        "            The default value is set to False (safe setting)\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "            it is a void method and returns the None value\n",
        "        \"\"\"\n",
        "        use_existing_index = True\n",
        "        if os.path.exists(indexpath):\n",
        "            if overwrite:\n",
        "                shutil.rmtree(indexpath)\n",
        "                use_existing_index = False\n",
        "        if not os.path.exists(indexpath):\n",
        "            os.mkdir(indexpath)\n",
        "            use_existing_index = False\n",
        "        self.pubmed_article_schema = Schema(\n",
        "            pmid=ID(stored=True),\n",
        "            title=TEXT(stored=True),\n",
        "            journal=TEXT(stored=True),\n",
        "            mesh_major=IDLIST(stored=True),\n",
        "            year=NUMERIC(stored=True),\n",
        "            abstract_text=TEXT(stored=True, analyzer=StemmingAnalyzer()))\n",
        "        print(use_existing_index)\n",
        "        if not use_existing_index:\n",
        "            self.pubmed_article_ix = index.create_in(\n",
        "                indexpath,\n",
        "                self.pubmed_article_schema,\n",
        "                indexname=\"pubmed_articles\")\n",
        "        else:\n",
        "            self.pubmed_article_ix = index.open_dir(\n",
        "                indexpath, indexname=\"pubmed_articles\")\n",
        "        print(\"index object created\")\n",
        "\n",
        "    def rm_index(self, indexpath: str = \"indexdir\") -> None:\n",
        "        \"\"\"\n",
        "        This is a utility function to delete an existing index\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        indexpath: str\n",
        "            The absolute or relative path of the index location\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "            This void medhod return nothing\n",
        "        \"\"\"\n",
        "        if os.path.exists(indexpath):\n",
        "            os.rmdir(indexpath)\n",
        "\n",
        "    def index_docs(self, articles,\n",
        "                   limit: int):\n",
        "        \"\"\"\"\n",
        "        indexes documents into the Whoosh index\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        articles: List[PubmedArticle]\n",
        "            The list of articles to be added to the index\n",
        "        limit: int\n",
        "            This is a cutoff, beyond which the indexing process will cease\n",
        "            The purpose of this parameter is to limit the amount of documents\n",
        "            to be indexed for testing purposes or quick function execution for\n",
        "            experimental methods\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None:\n",
        "           this is a void method an returns nothing\n",
        "\n",
        "        TODO: add handling LockError\n",
        "        TODO: add handling test for LockError\n",
        "        \"\"\"\n",
        "        print(\"adding documents\")\n",
        "        pubmed_article_writer = self.pubmed_article_ix.writer()\n",
        "        count = 0\n",
        "        for article in articles:\n",
        "            count += 1\n",
        "            if count > limit:\n",
        "                break\n",
        "            pubmed_article_writer.add_document(\n",
        "                pmid=article.pmid,\n",
        "                title=article.title,\n",
        "                journal=article.journal,\n",
        "                mesh_major=article.mesh_major,\n",
        "                year=article.year,\n",
        "                abstract_text=article.abstract_text)\n",
        "        pubmed_article_writer.commit()\n",
        "        print(\"commiting index, added\", count, \"documents\")\n",
        "\n",
        "    def search(self, query,\n",
        "               max_results: int = 1):\n",
        "        \"\"\"\n",
        "        This is our simple starter method to query the index\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        query: str\n",
        "           This is a plain text query string that Whoosh searches\n",
        "           the index for matches\n",
        "        max_results: int\n",
        "           This parameter sets the maximum number of results the\n",
        "           method will return\n",
        "        \"\"\"\n",
        "        res = []\n",
        "        qp = QueryParser(\"abstract_text\", schema=self.pubmed_article_schema)\n",
        "        q = qp.parse(query)\n",
        "        with self.pubmed_article_ix.searcher() as s:\n",
        "            results = s.search(q, limit=max_results)\n",
        "            for result in results:\n",
        "                pa = PubmedArticle(result['pmid'],\n",
        "                                   result['title'],\n",
        "                                   result['journal'],\n",
        "                                   result['year'],\n",
        "                                   result['abstract_text'],\n",
        "                                   result['mesh_major'])\n",
        "                res.append(pa)\n",
        "            return res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDrdqEHZXjgM"
      },
      "source": [
        "XML Extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJZ08yxXXlvC"
      },
      "source": [
        "import lxml.etree as ET\n",
        "\n",
        "def extract_and_write(filename, results, question_id, query):\n",
        "    \"\"\"\n",
        "    Extract information from IR system and write to XML file. Format is:\n",
        "    <Result PMID=1>\n",
        "        <Journal>Title of journal</Journal>\n",
        "        <Year>Year published</Year>\n",
        "        <Title>Title of article</Title>\n",
        "        <Abstract>Abstract (~couple of sentences/a paragraph)</Abstract>\n",
        "        <MERS>tag1</MERS>\n",
        "        <MERS>tag2</MERS>\n",
        "    </Result>\n",
        "    :param filename: Name of the XML file used in the QA system\n",
        "    \"\"\"\n",
        "    origTree = ET.parse(filename)\n",
        "    root = origTree.getroot()\n",
        "\n",
        "    Q = root.find(\"Q\")\n",
        "    IR = Q.find(\"IR\")\n",
        "\n",
        "    # Find the IR element to write to\n",
        "    questions = root.findall(\"Q\")\n",
        "    for question in questions:\n",
        "        if question.get(\"id\") == question_id:\n",
        "            IR = question.find(\"IR\")\n",
        "            # Create a subelement for each part of the result (there can be many)\n",
        "            for pa in results:\n",
        "              queryUsed = ET.SubElement(IR, \"QueryUsed\")\n",
        "              queryUsed.text = query\n",
        "              result = ET.SubElement(IR, \"Result\")\n",
        "              result.set(\"PMID\", pa.pmid)\n",
        "              journal = ET.SubElement(result, \"Journal\")\n",
        "              journal.text = pa.journal\n",
        "              year = ET.SubElement(result, \"Year\")\n",
        "              year.text = pa.year\n",
        "              title = ET.SubElement(result, \"Title\")\n",
        "              title.text = pa.title\n",
        "              abstract = ET.SubElement(result, \"Abstract\")\n",
        "              abstract.text = pa.abstract_text\n",
        "              for mesh in pa.mesh_major:\n",
        "                  mesh_major = ET.SubElement(result, \"MeSH\")\n",
        "                  mesh_major.text = mesh\n",
        "        tree = ET.ElementTree(root)\n",
        "        tree.write(filename, pretty_print=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSxqYQW5e649"
      },
      "source": [
        "Use exisiting index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj52maUONeZC"
      },
      "source": [
        "Copy index from drive into local colab files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znM39VeYNeJt"
      },
      "source": [
        "!mkdir indexdir\n",
        "!gsutil cp \"gdrive/My Drive/Colab Notebooks/BioASQ/indexdir/_pubmed_articles_1.toc\" \"indexdir\"\n",
        "!gsutil cp \"gdrive/My Drive/Colab Notebooks/BioASQ/indexdir/pubmed_articles_liyfs44zssrgfqtn.seg\" \"indexdir\"\n",
        "!gsutil cp \"gdrive/My Drive/Colab Notebooks/BioASQ/indexdir/pubmed_articles_WRITELOCK\" \"indexdir\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtU54QF37Mhi"
      },
      "source": [
        "# Use existing index in Google Drive\n",
        "pubmed_indexer = PubmedIndexer()\n",
        "pubmed_indexer.mk_index('indexdir', overwrite=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS8hOoEMe4AU"
      },
      "source": [
        "Run XML extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIV4eiz0eyEn"
      },
      "source": [
        "file = 'gdrive/My Drive/Colab Notebooks/BioASQ/qp_demo.xml'\n",
        "origTree = ET.parse(file)\n",
        "root = origTree.getroot()\n",
        "for question in root.findall('Q'):\n",
        "    # Question ID to write IR results to the appropriate question\n",
        "    qid = question.get(\"id\")\n",
        "    qp = question.find(\"QP\")\n",
        "\n",
        "    # If there is no query, use the original question\n",
        "    if qp.find(\"Query\").text:\n",
        "        query = qp.find(\"Query\").text\n",
        "    else:\n",
        "        query = question.text\n",
        "\n",
        "    results = pubmed_indexer.search(query)\n",
        "\n",
        "    # Only want to call write method if a result was found for the query\n",
        "    if results:\n",
        "      extract_and_write(file, results, qid, query)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}