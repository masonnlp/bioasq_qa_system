{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BioASQ_QA_System.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkA3LkTOy+D6tSAp13b/Bj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masonnlp/bioasq_qa_system/blob/master/BioASQ_QA_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Z4c1stH0Jy"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga-BTwK-ICcj"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install transformers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device=torch.device('cuda')\n",
        "from transformers import BertTokenizer,BertForSequenceClassification,AdamW,BertConfig,get_linear_schedule_with_warmup\n",
        "from lxml import etree as ET\n",
        "!pip3 install scispacy\n",
        "!pip3 install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
        "import spacy\n",
        "import scispacy\n",
        "import en_core_sci_lg\n",
        "nlp = en_core_sci_lg.load()\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ued1Px0fZhF1"
      },
      "source": [
        "Read input file (.csv) and predict type for each question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5FspDEzLT1i"
      },
      "source": [
        "def preprocess(df):\n",
        "  df.encoded_tokens = [tokenizer.encode_plus(text,add_special_tokens=True)['input_ids'] for text in df['Question']] #encoded tokens for each tweet\n",
        "  df.attention_mask = [tokenizer.encode_plus(text,add_special_tokens=True)['attention_mask'] for text in df['Question']]\n",
        "  encoded_tokens = list(df.encoded_tokens)\n",
        "  attention_mask = list(df.attention_mask)\n",
        "  return encoded_tokens,attention_mask\n",
        "\n",
        "# Convert indices to Torch tensor and dump into cuda\n",
        "def feed_generator(encoded_tokens,attention_mask):\n",
        "\n",
        "    batch_size = 16\n",
        "    batch_seq = [x for x in range(int(len(encoded_tokens)/batch_size))]\n",
        "\n",
        "\n",
        "    shuffled_encoded_tokens,shuffled_attention_mask = encoded_tokens,attention_mask\n",
        "\n",
        "    res = len(encoded_tokens)%batch_size\n",
        "    if res != 0:\n",
        "        batch_seq = [x for x in range(int(len(encoded_tokens)/batch_size)+1)]\n",
        "    shuffled_encoded_tokens = shuffled_encoded_tokens+shuffled_encoded_tokens[:res]\n",
        "    shuffled_attention_mask = shuffled_attention_mask+shuffled_attention_mask[:res]\n",
        "\n",
        "    for batch in batch_seq:\n",
        "        maxlen_sent = max([len(i) for i in shuffled_encoded_tokens[batch*batch_size:(batch+1)*batch_size]])\n",
        "        token_tensor = torch.tensor([tokens+[0]*(maxlen_sent-len(tokens)) for tokens in shuffled_encoded_tokens[batch*batch_size:(batch+1)*batch_size]])\n",
        "        attention_mask = torch.tensor([tokens+[0]*(maxlen_sent-len(tokens)) for tokens in shuffled_attention_mask[batch*batch_size:(batch+1)*batch_size]]) \n",
        "\n",
        "        token_tensor = token_tensor.to('cuda')\n",
        "        attention_mask = attention_mask.to('cuda')\n",
        "\n",
        "        yield token_tensor,attention_mask\n",
        "\n",
        "def predict(model,data):\n",
        "    model.eval()\n",
        "    model.cuda()\n",
        "    preds = []\n",
        "    batch_count = 0\n",
        "    for token_tensor, attention_mask in data:\n",
        "        with torch.no_grad():\n",
        "            logits = model(token_tensor,token_type_ids=None,attention_mask=attention_mask)[0]\n",
        "        tmp_preds = torch.argmax(logits,-1).detach().cpu().numpy().tolist()\n",
        "        preds += tmp_preds             \n",
        "    return preds\n",
        "\n",
        "test_data_path = '/content/gdrive/My Drive/Colab Notebooks/BioASQ/input.csv'\n",
        "testing_df = pd.read_csv(test_data_path,sep=',',header=0)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#checkpoint_dir = \"gdrive/My Drive/Colab Notebooks/bert-large-v3/\"\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('/content/gdrive/My Drive/Colab Notebooks/BioASQ/Model/', cache_dir=None)\n",
        "\n",
        "encoded_tokens_Test,attention_mask_Test = preprocess(testing_df)\n",
        "data_test = feed_generator(encoded_tokens_Test, attention_mask_Test)\n",
        "preds_test = predict(model,data_test)\n",
        "\n",
        "\n",
        "indices_to_label = {0: 'factoid', 1: 'list', 2: 'summary', 3: 'yesno'}\n",
        "\n",
        "predict_label = []\n",
        "for i in preds_test[0:len(testing_df['Question'])]:\n",
        "  for j in indices_to_label:\n",
        "    if i == j:\n",
        "      predict_label.append(indices_to_label[j])\n",
        "\n",
        "testing_df['type'] = predict_label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNqZoyumZs3R"
      },
      "source": [
        "Create output file (XML) providing question type to Answer Processing system and query for Information Retrieval system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR1mRNENGMWH"
      },
      "source": [
        "def xml_tree(df):\n",
        "    root = ET.Element(\"Input\")\n",
        "    for ind in df.index:\n",
        "      id = df['ID'][ind]\n",
        "      question = df['Question'][ind]\n",
        "      qtype = df['type'][ind]\n",
        "      q = ET.SubElement(root,\"Q\")\n",
        "      q.set('id',str(id))\n",
        "      q.text = question\n",
        "      qp = ET.SubElement(q,\"QP\")\n",
        "      qp_type = ET.SubElement(qp,'Type')\n",
        "      qp_type.text = qtype\n",
        "      doc = nlp(question)\n",
        "      ent_list = []\n",
        "      for ent in doc.ents:\n",
        "        ent_list.append(str(ent))\n",
        "        qp_en = ET.SubElement(qp,'Entities') \n",
        "        qp_en.text = str(ent)\n",
        "      qp_query = ET.SubElement(qp,'Query')\n",
        "      qp_query.text = str(' '.join(ent_list))\n",
        "      # Create IR tag\n",
        "      IR = ET.SubElement(q, \"IR\")\n",
        "      \n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write('gdrive/My Drive/Colab Notebooks/BioASQ/qp_demo.xml', pretty_print=True)\n",
        "    \n",
        "\n",
        "xml_tree(testing_df)    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QDvrSeKiuK0"
      },
      "source": [
        "**Start IR Module**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ay1g3DAV_eI"
      },
      "source": [
        "PubmedArticle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ5I_fgxWDvX"
      },
      "source": [
        "\"\"\"\n",
        "This module implements the class DataSetReader which contains\n",
        " the implementation of code to read the BioAsq dataset\n",
        "\"\"\"\n",
        "from typing import List\n",
        "\n",
        "class PubmedArticle:\n",
        "\n",
        "    def fromDict(data: dict):\n",
        "        pmid = data[\"pmid\"]\n",
        "        title = data[\"title\"]\n",
        "        journal = data[\"journal\"]\n",
        "        mesh_major = data[\"meshMajor\"]\n",
        "        year = data[\"year\"]\n",
        "        abstract_text = data[\"abstractText\"]\n",
        "        return PubmedArticle(pmid, title, journal,\n",
        "                             year, abstract_text, mesh_major)\n",
        "\n",
        "    def __init__(self, pmid: str, title: str, journal: str,\n",
        "                 year: str, abstract_text: str, mesh_major: List[str]):\n",
        "        self.journal = journal\n",
        "        self.mesh_major = mesh_major\n",
        "        self.year = year\n",
        "        self.abstract_text = abstract_text\n",
        "        self.pmid = pmid\n",
        "        self.title = title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od1-NjWrWO0i"
      },
      "source": [
        "PubmedReader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUw6z2-dWTT1"
      },
      "source": [
        "\"\"\"\n",
        "This modeule implements reading pubmed xml fragments\n",
        "\"\"\"\n",
        "import os\n",
        "import gzip\n",
        "import xml.etree.ElementTree as ET\n",
        "from typing import List\n",
        "# No need to import PubmedArticle since it's in the same notebook\n",
        "\n",
        "\n",
        "class PubmedReader:\n",
        "    \"\"\"\n",
        "    This class is responsible for reading the Pubmed dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        default constructor doesn't do anything\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def get_xml_frags(self, dir: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        given a directory where all the xml fragments reside\n",
        "        will return the list of all the xml fragments\n",
        "        \"\"\"\n",
        "        file_names = os.listdir(dir)\n",
        "        file_indexes = [i for i, val in enumerate(\n",
        "            map(lambda nm: nm.startswith(\"pubmed\")\n",
        "                and nm.endswith(\".xml.gz\"),\n",
        "                file_names)) if val]\n",
        "        return list(map(lambda i: file_names[i], file_indexes))\n",
        "\n",
        "    def process_xml_frags(\n",
        "            self, dir: str,\n",
        "            max_article_count: int):\n",
        "        frags = self.get_xml_frags(dir)\n",
        "        remaining_count = max_article_count\n",
        "        for frag in frags:\n",
        "            if remaining_count > 0:\n",
        "                articles = self.process_xml_frag(dir + \"/\"\n",
        "                                                 + frag, remaining_count)\n",
        "                remaining_count -= len(articles)\n",
        "                if len(articles) == 0:\n",
        "                    break\n",
        "                for article in articles:\n",
        "                    yield article\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    def process_xml_frag(\n",
        "            self, fname: str, max_article_count:\n",
        "            int):\n",
        "        \"\"\"\n",
        "        This method reads to a complete gzipped xml file\n",
        "        and extracts each PubmedArticle, and returns a list\n",
        "        of PubmedArticle objects that contain all the relevant\n",
        "        fields\n",
        "        \"\"\"\n",
        "        articles = []\n",
        "        with gzip.open(fname, 'rt', encoding=\"utf-8\") as f:\n",
        "            count = 0\n",
        "            pubmed_article_txt = \"\"\n",
        "            record = False\n",
        "            while True:\n",
        "                line = f.readline()\n",
        "                if not line:\n",
        "                    break\n",
        "                if '<PubmedArticle>' in line:\n",
        "                    record = True\n",
        "                if record:\n",
        "                    pubmed_article_txt += line\n",
        "                if '</PubmedArticle>' in line:\n",
        "                    if count >= max_article_count:\n",
        "                        print(\"reached max article count ending read\")\n",
        "                        break\n",
        "                    count += 1\n",
        "                    record = False\n",
        "                    articles.append(\n",
        "                        self.process_pubmed_article_xml(pubmed_article_txt))\n",
        "                    pubmed_article_txt = \"\"\n",
        "        print(\"fname\", fname, \"articles\", count)\n",
        "        return articles\n",
        "\n",
        "    def process_pubmed_article_xml(self, txt: str) -> PubmedArticle:\n",
        "        \"\"\"\n",
        "        this article takes an XML fragment of a single Pubmed article\n",
        "        entry and parses it for data\n",
        "        It returns a populated PubmedArticle object\n",
        "        \"\"\"\n",
        "        root = ET.fromstring(txt)\n",
        "        pmid = root.findtext('.//PMID')\n",
        "        title = root.findtext('.//ArticleTitle')\n",
        "        abstract_text = root.findtext('.//AbstractText')\n",
        "        journal = root.findtext('.//Title')\n",
        "        if root.findtext('.//PubDate/Year'):\n",
        "          year = root.findtext('.//PubDate/Year')\n",
        "        else:\n",
        "          year = 0000\n",
        "        mesh_major = list(\n",
        "            map(lambda x: x.text, root.findall(\".//DescriptorName\")))\n",
        "        return PubmedArticle(\n",
        "            pmid, title, journal, year, abstract_text, mesh_major)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVtd25juWzG8"
      },
      "source": [
        "PubmedIndexer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTsSBSqeXViN"
      },
      "source": [
        "Install Whoosh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6WYS7GBXUEs"
      },
      "source": [
        "!pip install whoosh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTXSjCpsW3oy"
      },
      "source": [
        "\"\"\"\n",
        "This module indexes the Pubmed dataset using Whoosh\n",
        "\"\"\"\n",
        "import os\n",
        "import os.path\n",
        "import shutil\n",
        "from whoosh import index\n",
        "from whoosh.fields import Schema, TEXT, IDLIST, ID, NUMERIC\n",
        "from whoosh.analysis import StemmingAnalyzer\n",
        "from whoosh.qparser import QueryParser\n",
        "from datetime import datetime\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class PubmedIndexer:\n",
        "    \"\"\"\n",
        "    PubmedIndexer is the main class that clients are expected to to use.\n",
        "    The primary functions it performs are:\n",
        "    1. Indexing the pubmed articles into a Whoosh index\n",
        "    2. Allowing the free text searching of the pubmed articles\n",
        "\n",
        "    NOTES:\n",
        "    1. The pubmed data is provided here:\n",
        "      ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/\n",
        "    2. We do not index all the fields per article -- we index:\n",
        "      a. The pubmed ID\n",
        "      b. The Journal name\n",
        "      c. The Year of publication\n",
        "      d. The Article title\n",
        "      e. The Article Abstract\n",
        "    3. The complete pubmed dataset is just under 7 GB of compressed\n",
        "      XML shards (as of this writing)\n",
        "    4. This module allows all this data to be indexed\n",
        "    5. The index takes about 5 hours to generate on a medium powered laptop\n",
        "    6. The index directly is roughly 7 GB\n",
        "    7. The index directory can be tarred(zipped) and shared between users\n",
        "    8. We will probably rename this module pubmed_ir soon and relase it to PyPI\n",
        "\n",
        "    MISSING & DESIRABLE FUNCTIONALITY\n",
        "    1. It would be good to have utility function that is able to download\n",
        "      the pub med data\n",
        "    2. We should get __init__.py, etc. files done so we can publish to PyPi\n",
        "    3. We should have a partial indexing feature that indexes only data needed\n",
        "       for biosqr task b\n",
        "    4. We might make the index generation system more customization interms\n",
        "       of things such as Analyzers, stop-words, etc.\n",
        "    5. We may need a customizable result scoring function -- beyond BM25\n",
        "    6. We may want a more sophisticated querying interface, boolean queries, etc\n",
        "    7. We need a lot of testing to certify the system\n",
        "    8. It is not clear if we can add documents to an existing index\n",
        "    9. It is not clear how we can re-index an existing index\n",
        "    10. We should swap out prints with a formal logging framework\n",
        "    11. We should have example modules which demonstrate the use of this system\n",
        "    12. We really need to modify the directory structure of the project\n",
        "\n",
        "    BUGS & KNOWN LIMITATIONS\n",
        "    1. At the moment the free text query only searches the Abstract Text\n",
        "      it does not search the title\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        default construstor it does nothing at the moment\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def mk_index(self, indexpath: str = \"indexdir\",\n",
        "                 overwrite: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        creates a Whoosh based index for subsequent IR operatons\n",
        "\n",
        "        Prameters\n",
        "        ---------\n",
        "        indexpath: str\n",
        "            The absolute or relative path where you want the index to be stored\n",
        "               Note: the index path is a directory\n",
        "               this directory will contain all the Whoosh files\n",
        "        overwrite: boolean\n",
        "            This will overwrite any existing index (directory) if set to True\n",
        "            The default value is set to False (safe setting)\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "            it is a void method and returns the None value\n",
        "        \"\"\"\n",
        "        use_existing_index = True\n",
        "        if os.path.exists(indexpath):\n",
        "            if overwrite:\n",
        "                shutil.rmtree(indexpath)\n",
        "                use_existing_index = False\n",
        "        if not os.path.exists(indexpath):\n",
        "            os.mkdir(indexpath)\n",
        "            use_existing_index = False\n",
        "        self.pubmed_article_schema = Schema(\n",
        "            pmid=ID(stored=True),\n",
        "            title=TEXT(stored=True),\n",
        "            journal=TEXT(stored=True),\n",
        "            mesh_major=IDLIST(stored=True),\n",
        "            year=NUMERIC(stored=True),\n",
        "            abstract_text=TEXT(stored=True, analyzer=StemmingAnalyzer()))\n",
        "        print(use_existing_index)\n",
        "        if not use_existing_index:\n",
        "            self.pubmed_article_ix = index.create_in(\n",
        "                indexpath,\n",
        "                self.pubmed_article_schema,\n",
        "                indexname=\"pubmed_articles\")\n",
        "        else:\n",
        "            self.pubmed_article_ix = index.open_dir(\n",
        "                indexpath, indexname=\"pubmed_articles\")\n",
        "        print(\"index object created\")\n",
        "\n",
        "    def rm_index(self, indexpath: str = \"indexdir\") -> None:\n",
        "        \"\"\"\n",
        "        This is a utility function to delete an existing index\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        indexpath: str\n",
        "            The absolute or relative path of the index location\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "            This void medhod return nothing\n",
        "        \"\"\"\n",
        "        if os.path.exists(indexpath):\n",
        "            os.rmdir(indexpath)\n",
        "\n",
        "    def index_docs(self, articles,\n",
        "                   limit: int):\n",
        "        \"\"\"\"\n",
        "        indexes documents into the Whoosh index\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        articles: List[PubmedArticle]\n",
        "            The list of articles to be added to the index\n",
        "        limit: int\n",
        "            This is a cutoff, beyond which the indexing process will cease\n",
        "            The purpose of this parameter is to limit the amount of documents\n",
        "            to be indexed for testing purposes or quick function execution for\n",
        "            experimental methods\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None:\n",
        "           this is a void method an returns nothing\n",
        "\n",
        "        TODO: add handling LockError\n",
        "        TODO: add handling test for LockError\n",
        "        \"\"\"\n",
        "        print(\"adding documents\")\n",
        "        pubmed_article_writer = self.pubmed_article_ix.writer()\n",
        "        count = 0\n",
        "        for article in articles:\n",
        "            count += 1\n",
        "            if count > limit:\n",
        "                break\n",
        "            pubmed_article_writer.add_document(\n",
        "                pmid=article.pmid,\n",
        "                title=article.title,\n",
        "                journal=article.journal,\n",
        "                mesh_major=article.mesh_major,\n",
        "                year=article.year,\n",
        "                abstract_text=article.abstract_text)\n",
        "        pubmed_article_writer.commit()\n",
        "        print(\"commiting index, added\", count, \"documents\")\n",
        "\n",
        "    def search(self, query,\n",
        "               max_results: int = 1):\n",
        "        \"\"\"\n",
        "        This is our simple starter method to query the index\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        query: str\n",
        "           This is a plain text query string that Whoosh searches\n",
        "           the index for matches\n",
        "        max_results: int\n",
        "           This parameter sets the maximum number of results the\n",
        "           method will return\n",
        "        \"\"\"\n",
        "        res = []\n",
        "        qp = QueryParser(\"abstract_text\", schema=self.pubmed_article_schema)\n",
        "        q = qp.parse(query)\n",
        "        with self.pubmed_article_ix.searcher() as s:\n",
        "            results = s.search(q, limit=max_results)\n",
        "            for result in results:\n",
        "                pa = PubmedArticle(result['pmid'],\n",
        "                                   result['title'],\n",
        "                                   result['journal'],\n",
        "                                   result['year'],\n",
        "                                   result['abstract_text'],\n",
        "                                   result['mesh_major'])\n",
        "                res.append(pa)\n",
        "            return res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDrdqEHZXjgM"
      },
      "source": [
        "XML Extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJZ08yxXXlvC"
      },
      "source": [
        "import lxml.etree as ET\n",
        "\n",
        "def extract_and_write(filename, results, question_id, query):\n",
        "    \"\"\"\n",
        "    Extract information from IR system and write to XML file. Format is:\n",
        "    <Result PMID=1>\n",
        "        <Journal>Title of journal</Journal>\n",
        "        <Year>Year published</Year>\n",
        "        <Title>Title of article</Title>\n",
        "        <Abstract>Abstract (~couple of sentences/a paragraph)</Abstract>\n",
        "        <MERS>tag1</MERS>\n",
        "        <MERS>tag2</MERS>\n",
        "    </Result>\n",
        "    :param filename: Name of the XML file used in the QA system\n",
        "    \"\"\"\n",
        "    origTree = ET.parse(filename)\n",
        "    root = origTree.getroot()\n",
        "\n",
        "    Q = root.find(\"Q\")\n",
        "    IR = Q.find(\"IR\")\n",
        "\n",
        "    # Find the IR element to write to\n",
        "    questions = root.findall(\"Q\")\n",
        "    for question in questions:\n",
        "        if question.get(\"id\") == question_id:\n",
        "            IR = question.find(\"IR\")\n",
        "            # Create a subelement for each part of the result (there can be many)\n",
        "            for pa in results:\n",
        "              queryUsed = ET.SubElement(IR, \"QueryUsed\")\n",
        "              queryUsed.text = query\n",
        "              result = ET.SubElement(IR, \"Result\")\n",
        "              result.set(\"PMID\", pa.pmid)\n",
        "              journal = ET.SubElement(result, \"Journal\")\n",
        "              journal.text = pa.journal\n",
        "              year = ET.SubElement(result, \"Year\")\n",
        "              year.text = pa.year\n",
        "              title = ET.SubElement(result, \"Title\")\n",
        "              title.text = pa.title\n",
        "              abstract = ET.SubElement(result, \"Abstract\")\n",
        "              abstract.text = pa.abstract_text\n",
        "              for mesh in pa.mesh_major:\n",
        "                  mesh_major = ET.SubElement(result, \"MeSH\")\n",
        "                  mesh_major.text = mesh\n",
        "        tree = ET.ElementTree(root)\n",
        "        tree.write(filename, pretty_print=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSxqYQW5e649"
      },
      "source": [
        "Index Documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtU54QF37Mhi"
      },
      "source": [
        "# Create new index\n",
        "pubmed_indexer = PubmedIndexer()\n",
        "pubmed_indexer.mk_index('indexdir2', overwrite=True)\n",
        "reader = PubmedReader()\n",
        "articles = reader.process_xml_frags('gdrive/My Drive/Colab Notebooks/BioASQ/data2', max_article_count=10000)\n",
        "pubmed_indexer.index_docs(articles, limit=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS8hOoEMe4AU"
      },
      "source": [
        "Run XML extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIV4eiz0eyEn"
      },
      "source": [
        "file = 'gdrive/My Drive/Colab Notebooks/BioASQ/qp_demo.xml'\n",
        "origTree = ET.parse(file)\n",
        "root = origTree.getroot()\n",
        "for question in root.findall('Q'):\n",
        "    # Question ID to write IR results to the appropriate question\n",
        "    qid = question.get(\"id\")\n",
        "    qp = question.find(\"QP\")\n",
        "\n",
        "    # If there is no query, use the original question\n",
        "    if qp.find(\"Query\").text:\n",
        "        query = qp.find(\"Query\").text\n",
        "    else:\n",
        "        query = question.text\n",
        "\n",
        "    results = pubmed_indexer.search(query)\n",
        "\n",
        "    # Only want to call write method if a result was found for the query\n",
        "    if results:\n",
        "      extract_and_write(file, results, qid, query)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}