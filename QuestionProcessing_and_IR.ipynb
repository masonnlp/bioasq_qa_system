{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionProcessing_and_IR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masonnlp/bioasq_qa_system/blob/master/QuestionProcessing_and_IR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Z4c1stH0Jy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a82fe4b-afa3-480d-eb6c-8426fdc91183"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga-BTwK-ICcj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14666b3a-f1fa-4215-e064-58961a3a4bb0"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install transformers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device=torch.device('cuda')\n",
        "from transformers import BertTokenizer,BertForSequenceClassification,AdamW,BertConfig,get_linear_schedule_with_warmup\n",
        "from lxml import etree as ET\n",
        "!pip3 install scispacy\n",
        "!pip3 install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
        "import spacy\n",
        "import scispacy\n",
        "import en_core_sci_lg\n",
        "nlp = en_core_sci_lg.load()\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: scispacy in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.6/dist-packages (from scispacy) (0.3.2)\n",
            "Requirement already satisfied: requests<3.0.0conllu,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scispacy) (2.23.0)\n",
            "Requirement already satisfied: nmslib>=1.7.3.6 in /usr/local/lib/python3.6/dist-packages (from scispacy) (2.0.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from scispacy) (0.16.0)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scispacy) (2.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scispacy) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from scispacy) (0.22.2.post1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0conllu,>=2.0.0->scispacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0conllu,>=2.0.0->scispacy) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0conllu,>=2.0.0->scispacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0conllu,>=2.0.0->scispacy) (3.0.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from nmslib>=1.7.3.6->scispacy) (5.4.8)\n",
            "Requirement already satisfied: pybind11>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from nmslib>=1.7.3.6->scispacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (7.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (50.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.3.0->scispacy) (2.0.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.3->scispacy) (1.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.0->scispacy) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.0->scispacy) (3.1.0)\n",
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
            "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
            "Requirement already satisfied (use --upgrade to upgrade): en-core-sci-lg==0.2.4 from https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: spacy>=2.2.1 in /usr/local/lib/python3.6/dist-packages (from en-core-sci-lg==0.2.4) (2.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (50.3.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (7.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-core-sci-lg==0.2.4) (0.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-core-sci-lg==0.2.4) (2020.6.20)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-core-sci-lg==0.2.4) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-core-sci-lg==0.2.4) (3.1.0)\n",
            "Building wheels for collected packages: en-core-sci-lg\n",
            "  Building wheel for en-core-sci-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-sci-lg: filename=en_core_sci_lg-0.2.4-cp36-none-any.whl size=501343162 sha256=f1c63429dd14f5a2b5d9a1e0f0eddb30d26d55a677232be340384e368bd34e3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/ab/e5/fa667519032799529ce6a50944a82d6ae3603819cd07836aa2\n",
            "Successfully built en-core-sci-lg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_sci_lg' (0.2.4) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ued1Px0fZhF1",
        "colab_type": "text"
      },
      "source": [
        "Read input file (.csv) and predict type for each question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5FspDEzLT1i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "c383a7e3-49d3-4388-97d3-d962a2bd54cd"
      },
      "source": [
        "def preprocess(df):\n",
        "  df.encoded_tokens = [tokenizer.encode_plus(text,add_special_tokens=True)['input_ids'] for text in df['Question']] #encoded tokens for each tweet\n",
        "  df.attention_mask = [tokenizer.encode_plus(text,add_special_tokens=True)['attention_mask'] for text in df['Question']]\n",
        "  encoded_tokens = list(df.encoded_tokens)\n",
        "  attention_mask = list(df.attention_mask)\n",
        "  return encoded_tokens,attention_mask\n",
        "\n",
        "# Convert indices to Torch tensor and dump into cuda\n",
        "def feed_generator(encoded_tokens,attention_mask):\n",
        "\n",
        "    batch_size = 16\n",
        "    batch_seq = [x for x in range(int(len(encoded_tokens)/batch_size))]\n",
        "\n",
        "\n",
        "    shuffled_encoded_tokens,shuffled_attention_mask = encoded_tokens,attention_mask\n",
        "\n",
        "    res = len(encoded_tokens)%batch_size\n",
        "    if res != 0:\n",
        "        batch_seq = [x for x in range(int(len(encoded_tokens)/batch_size)+1)]\n",
        "    shuffled_encoded_tokens = shuffled_encoded_tokens+shuffled_encoded_tokens[:res]\n",
        "    shuffled_attention_mask = shuffled_attention_mask+shuffled_attention_mask[:res]\n",
        "\n",
        "    for batch in batch_seq:\n",
        "        maxlen_sent = max([len(i) for i in shuffled_encoded_tokens[batch*batch_size:(batch+1)*batch_size]])\n",
        "        token_tensor = torch.tensor([tokens+[0]*(maxlen_sent-len(tokens)) for tokens in shuffled_encoded_tokens[batch*batch_size:(batch+1)*batch_size]])\n",
        "        attention_mask = torch.tensor([tokens+[0]*(maxlen_sent-len(tokens)) for tokens in shuffled_attention_mask[batch*batch_size:(batch+1)*batch_size]]) \n",
        "\n",
        "        token_tensor = token_tensor.to('cuda')\n",
        "        attention_mask = attention_mask.to('cuda')\n",
        "\n",
        "        yield token_tensor,attention_mask\n",
        "\n",
        "def predict(model,data):\n",
        "    model.eval()\n",
        "    model.cuda()\n",
        "    preds = []\n",
        "    batch_count = 0\n",
        "    for token_tensor, attention_mask in data:\n",
        "        with torch.no_grad():\n",
        "            logits = model(token_tensor,token_type_ids=None,attention_mask=attention_mask)[0]\n",
        "        tmp_preds = torch.argmax(logits,-1).detach().cpu().numpy().tolist()\n",
        "        preds += tmp_preds             \n",
        "    return preds\n",
        "\n",
        "test_data_path = '/content/gdrive/My Drive/Colab Notebooks/BioASQ/input.csv'\n",
        "testing_df = pd.read_csv(test_data_path,sep=',',header=0)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#checkpoint_dir = \"gdrive/My Drive/Colab Notebooks/bert-large-v3/\"\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('/content/gdrive/My Drive/Colab Notebooks/BioASQ/Model/model 1/', cache_dir=None)\n",
        "\n",
        "encoded_tokens_Test,attention_mask_Test = preprocess(testing_df)\n",
        "data_test = feed_generator(encoded_tokens_Test, attention_mask_Test)\n",
        "preds_test = predict(model,data_test)\n",
        "\n",
        "\n",
        "indices_to_label = {0: 'factoid', 1: 'list', 2: 'summary', 3: 'yesno'}\n",
        "\n",
        "predict_label = []\n",
        "for i in preds_test[0:len(testing_df['Question'])]:\n",
        "  for j in indices_to_label:\n",
        "    if i == j:\n",
        "      predict_label.append(indices_to_label[j])\n",
        "\n",
        "testing_df['type'] = predict_label\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNqZoyumZs3R",
        "colab_type": "text"
      },
      "source": [
        "Create output file (XML) providing question type to Answer Processing system and query for Information Retrieval system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR1mRNENGMWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xml_tree(df):\n",
        "    root = ET.Element(\"Input\")\n",
        "    for ind in df.index:\n",
        "      id = df['ID'][ind]\n",
        "      question = df['Question'][ind]\n",
        "      qtype = df['type'][ind]\n",
        "      q = ET.SubElement(root,\"Q\")\n",
        "      q.set('id',str(id))\n",
        "      q.text = question\n",
        "      qp = ET.SubElement(q,\"QP\")\n",
        "      qp_type = ET.SubElement(qp,'Type')\n",
        "      qp_type.text = qtype\n",
        "      doc = nlp(question)\n",
        "      ent_list = []\n",
        "      for ent in doc.ents:\n",
        "        ent_list.append(str(ent))\n",
        "        qp_en = ET.SubElement(qp,'Entities') \n",
        "        qp_en.text = str(ent)\n",
        "      qp_query = ET.SubElement(qp,'Query')\n",
        "      qp_query.text = str(' '.join(ent_list))\n",
        "      # Create IR tag\n",
        "      IR = ET.SubElement(q, \"IR\")\n",
        "      \n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write('gdrive/My Drive/Colab Notebooks/BioASQ/qp_demo.xml', pretty_print=True)\n",
        "    \n",
        "\n",
        "xml_tree(testing_df)    \n",
        "    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QDvrSeKiuK0",
        "colab_type": "text"
      },
      "source": [
        "**Start IR Module**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ay1g3DAV_eI",
        "colab_type": "text"
      },
      "source": [
        "PubmedArticle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ5I_fgxWDvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This module implements the class DataSetReader which contains\n",
        " the implementation of code to read the BioAsq dataset\n",
        "\"\"\"\n",
        "from typing import List\n",
        "\n",
        "class PubmedArticle:\n",
        "\n",
        "    def fromDict(data: dict):\n",
        "        pmid = data[\"pmid\"]\n",
        "        title = data[\"title\"]\n",
        "        journal = data[\"journal\"]\n",
        "        mesh_major = data[\"meshMajor\"]\n",
        "        year = data[\"year\"]\n",
        "        abstract_text = data[\"abstractText\"]\n",
        "        return PubmedArticle(pmid, title, journal,\n",
        "                             year, abstract_text, mesh_major)\n",
        "\n",
        "    def __init__(self, pmid: str, title: str, journal: str,\n",
        "                 year: str, abstract_text: str, mesh_major: List[str]):\n",
        "        self.journal = journal\n",
        "        self.mesh_major = mesh_major\n",
        "        self.year = year\n",
        "        self.abstract_text = abstract_text\n",
        "        self.pmid = pmid\n",
        "        self.title = title"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od1-NjWrWO0i",
        "colab_type": "text"
      },
      "source": [
        "PubmedReader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUw6z2-dWTT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This modeule implements reading pubmed xml fragments\n",
        "\"\"\"\n",
        "import os\n",
        "import gzip\n",
        "import xml.etree.ElementTree as ET\n",
        "from typing import List\n",
        "# No need to import PubmedArticle since it's in the same notebook\n",
        "\n",
        "\n",
        "class PubmedReader:\n",
        "    \"\"\"\n",
        "    This class is responsible for reading the Pubmed dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        default constructor doesn't do anything\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def get_xml_frags(self, dir: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        given a directory where all the xml fragments reside\n",
        "        will return the list of all the xml fragments\n",
        "        \"\"\"\n",
        "        file_names = os.listdir(dir)\n",
        "        file_indexes = [i for i, val in enumerate(\n",
        "            map(lambda nm: nm.startswith(\"pubmed\")\n",
        "                and nm.endswith(\".xml.gz\"),\n",
        "                file_names)) if val]\n",
        "        return list(map(lambda i: file_names[i], file_indexes))\n",
        "\n",
        "    def process_xml_frags(\n",
        "            self, dir: str,\n",
        "            max_article_count: int = 10000000):\n",
        "        frags = self.get_xml_frags(dir)\n",
        "        remaining_count = max_article_count\n",
        "        for frag in frags:\n",
        "            if remaining_count > 0:\n",
        "                articles = self.process_xml_frag(dir + \"/\"\n",
        "                                                 + frag, remaining_count)\n",
        "                remaining_count -= len(articles)\n",
        "                if len(articles) == 0:\n",
        "                    break\n",
        "                for article in articles:\n",
        "                    yield article\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    def process_xml_frag(\n",
        "            self, fname: str, max_article_count:\n",
        "            int = 10000000):\n",
        "        \"\"\"\n",
        "        This method reads to a complete gzipped xml file\n",
        "        and extracts each PubmedArticle, and returns a list\n",
        "        of PubmedArticle objects that contain all the relevant\n",
        "        fields\n",
        "        \"\"\"\n",
        "        articles = []\n",
        "        with gzip.open(fname, 'rt', encoding=\"utf-8\") as f:\n",
        "            count = 0\n",
        "            pubmed_article_txt = \"\"\n",
        "            record = False\n",
        "            while True:\n",
        "                line = f.readline()\n",
        "                if not line:\n",
        "                    break\n",
        "                if '<PubmedArticle>' in line:\n",
        "                    record = True\n",
        "                if record:\n",
        "                    pubmed_article_txt += line\n",
        "                if '</PubmedArticle>' in line:\n",
        "                    if count >= max_article_count:\n",
        "                        print(\"reached max article count ending read\")\n",
        "                        break\n",
        "                    count += 1\n",
        "                    record = False\n",
        "                    articles.append(\n",
        "                        self.process_pubmed_article_xml(pubmed_article_txt))\n",
        "                    pubmed_article_txt = \"\"\n",
        "        print(\"fname\", fname, \"articles\", count)\n",
        "        return articles\n",
        "\n",
        "    def process_pubmed_article_xml(self, txt: str) -> PubmedArticle:\n",
        "        \"\"\"\n",
        "        this article takes an XML fragment of a single Pubmed article\n",
        "        entry and parses it for data\n",
        "        It returns a populated PubmedArticle object\n",
        "        \"\"\"\n",
        "        root = ET.fromstring(txt)\n",
        "        pmid = root.findtext('.//PMID')\n",
        "        title = root.findtext('.//ArticleTitle')\n",
        "        abstract_text = root.findtext('.//AbstractText')\n",
        "        journal = root.findtext('.//Title')\n",
        "        year = root.findtext('.//PubDate/Year')\n",
        "        mesh_major = list(\n",
        "            map(lambda x: x.text, root.findall(\".//DescriptorName\")))\n",
        "        return PubmedArticle(\n",
        "            pmid, title, journal, year, abstract_text, mesh_major)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVtd25juWzG8",
        "colab_type": "text"
      },
      "source": [
        "PubmedIndexer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTsSBSqeXViN",
        "colab_type": "text"
      },
      "source": [
        "Install Whoosh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6WYS7GBXUEs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2a2ede1-3399-40ce-fde8-12488a03ba22"
      },
      "source": [
        "!pip install whoosh"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: whoosh in /usr/local/lib/python3.6/dist-packages (2.7.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTXSjCpsW3oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This module indexes the Pubmed dataset using Whoosh\n",
        "\"\"\"\n",
        "import os\n",
        "import os.path\n",
        "import shutil\n",
        "from whoosh import index\n",
        "from whoosh.fields import Schema, TEXT, IDLIST, ID, NUMERIC\n",
        "from whoosh.analysis import StemmingAnalyzer\n",
        "from whoosh.qparser import QueryParser\n",
        "from datetime import datetime\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class PubmedIndexer:\n",
        "    \"\"\"\n",
        "    PubmedIndexer is the main class that clients are expected to to use.\n",
        "    The primary functions it performs are:\n",
        "    1. Indexing the pubmed articles into a Whoosh index\n",
        "    2. Allowing the free text searching of the pubmed articles\n",
        "\n",
        "    NOTES:\n",
        "    1. The pubmed data is provided here:\n",
        "      ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/\n",
        "    2. We do not index all the fields per article -- we index:\n",
        "      a. The pubmed ID\n",
        "      b. The Journal name\n",
        "      c. The Year of publication\n",
        "      d. The Article title\n",
        "      e. The Article Abstract\n",
        "    3. The complete pubmed dataset is just under 7 GB of compressed\n",
        "      XML shards (as of this writing)\n",
        "    4. This module allows all this data to be indexed\n",
        "    5. The index takes about 5 hours to generate on a medium powered laptop\n",
        "    6. The index directly is roughly 7 GB\n",
        "    7. The index directory can be tarred(zipped) and shared between users\n",
        "    8. We will probably rename this module pubmed_ir soon and relase it to PyPI\n",
        "\n",
        "    MISSING & DESIRABLE FUNCTIONALITY\n",
        "    1. It would be good to have utility function that is able to download\n",
        "      the pub med data\n",
        "    2. We should get __init__.py, etc. files done so we can publish to PyPi\n",
        "    3. We should have a partial indexing feature that indexes only data needed\n",
        "       for biosqr task b\n",
        "    4. We might make the index generation system more customization interms\n",
        "       of things such as Analyzers, stop-words, etc.\n",
        "    5. We may need a customizable result scoring function -- beyond BM25\n",
        "    6. We may want a more sophisticated querying interface, boolean queries, etc\n",
        "    7. We need a lot of testing to certify the system\n",
        "    8. It is not clear if we can add documents to an existing index\n",
        "    9. It is not clear how we can re-index an existing index\n",
        "    10. We should swap out prints with a formal logging framework\n",
        "    11. We should have example modules which demonstrate the use of this system\n",
        "    12. We really need to modify the directory structure of the project\n",
        "\n",
        "    BUGS & KNOWN LIMITATIONS\n",
        "    1. At the moment the free text query only searches the Abstract Text\n",
        "      it does not search the title\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        default construstor it does nothing at the moment\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def mk_index(self, indexpath: str = \"indexdir\",\n",
        "                 overwrite: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        creates a Whoosh based index for subsequent IR operatons\n",
        "\n",
        "        Prameters\n",
        "        ---------\n",
        "        indexpath: str\n",
        "            The absolute or relative path where you want the index to be stored\n",
        "               Note: the index path is a directory\n",
        "               this directory will contain all the Whoosh files\n",
        "        overwrite: boolean\n",
        "            This will overwrite any existing index (directory) if set to True\n",
        "            The default value is set to False (safe setting)\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "            it is a void method and returns the None value\n",
        "        \"\"\"\n",
        "        use_existing_index = True\n",
        "        if os.path.exists(indexpath):\n",
        "            if overwrite:\n",
        "                shutil.rmtree(indexpath)\n",
        "                use_existing_index = False\n",
        "        if not os.path.exists(indexpath):\n",
        "            os.mkdir(indexpath)\n",
        "            use_existing_index = False\n",
        "        self.pubmed_article_schema = Schema(\n",
        "            pmid=ID(stored=True),\n",
        "            title=TEXT(stored=True),\n",
        "            journal=TEXT(stored=True),\n",
        "            mesh_major=IDLIST(stored=True),\n",
        "            year=NUMERIC(stored=True),\n",
        "            abstract_text=TEXT(stored=True, analyzer=StemmingAnalyzer()))\n",
        "        print(use_existing_index)\n",
        "        if not use_existing_index:\n",
        "            self.pubmed_article_ix = index.create_in(\n",
        "                indexpath,\n",
        "                self.pubmed_article_schema,\n",
        "                indexname=\"pubmed_articles\")\n",
        "        else:\n",
        "            self.pubmed_article_ix = index.open_dir(\n",
        "                indexpath, indexname=\"pubmed_articles\")\n",
        "        print(\"index object created\")\n",
        "\n",
        "    def rm_index(self, indexpath: str = \"indexdir\") -> None:\n",
        "        \"\"\"\n",
        "        This is a utility function to delete an existing index\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        indexpath: str\n",
        "            The absolute or relative path of the index location\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "            This void medhod return nothing\n",
        "        \"\"\"\n",
        "        if os.path.exists(indexpath):\n",
        "            os.rmdir(indexpath)\n",
        "\n",
        "    def index_docs(self, articles,\n",
        "                   limit: int = 10000000):\n",
        "        \"\"\"\"\n",
        "        indexes documents into the Whoosh index\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        articles: List[PubmedArticle]\n",
        "            The list of articles to be added to the index\n",
        "        limit: init\n",
        "            This is a cutoff, beyond which the indexing process will cease\n",
        "            The purpose of this parameter is to limit the amount of documents\n",
        "            to be indexed for testing purposes or quick function execution for\n",
        "            experimental methods\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None:\n",
        "           this is a void method an returns nothing\n",
        "\n",
        "        TODO: add handling LockError\n",
        "        TODO: add handling test for LockError\n",
        "        \"\"\"\n",
        "        print(\"adding documents\")\n",
        "        pubmed_article_writer = self.pubmed_article_ix.writer()\n",
        "        count = 0\n",
        "        for article in articles:\n",
        "            count += 1\n",
        "            if count > limit:\n",
        "                break\n",
        "            pubmed_article_writer.add_document(\n",
        "                pmid=article.pmid,\n",
        "                title=article.title,\n",
        "                journal=article.journal,\n",
        "                mesh_major=article.mesh_major,\n",
        "                year=article.year,\n",
        "                abstract_text=article.abstract_text)\n",
        "        pubmed_article_writer.commit()\n",
        "        print(\"commiting index, added\", count, \"documents\")\n",
        "\n",
        "    def search(self, query,\n",
        "               max_results: int = 10):\n",
        "        \"\"\"\n",
        "        This is our simple starter method to query the index\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        query: str\n",
        "           This is a plain text query string that Whoosh searches\n",
        "           the index for matches\n",
        "        max_results: int\n",
        "           This parameter sets the maximum number of results the\n",
        "           method will return\n",
        "        \"\"\"\n",
        "        res = []\n",
        "        qp = QueryParser(\"abstract_text\", schema=self.pubmed_article_schema)\n",
        "        q = qp.parse(query)\n",
        "        with self.pubmed_article_ix.searcher() as s:\n",
        "            results = s.search(q, limit=max_results)\n",
        "            #PubmedIndexer.write_results(results)\n",
        "            for result in results:\n",
        "                pa = PubmedArticle(result['pmid'],\n",
        "                                   result['title'],\n",
        "                                   result['journal'],\n",
        "                                   result['year'],\n",
        "                                   result['abstract_text'],\n",
        "                                   result['mesh_major'])\n",
        "                res.append(pa)\n",
        "            return res\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDrdqEHZXjgM",
        "colab_type": "text"
      },
      "source": [
        "XML Extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJZ08yxXXlvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lxml.etree as ET\n",
        "\n",
        "def extract_and_write(filename):\n",
        "    \"\"\"\n",
        "    Extract information from IR system and write to XML file. Format is:\n",
        "    <Result PMID=1>\n",
        "       <Journal>Title of journal</Journal>\n",
        "       <Year>Year published</Year>\n",
        "       <Title>Title of article</Title>\n",
        "       <Abstract>Abstract (~couple of sentences/a paragraph)</Abstract>\n",
        "       <MERS>tag1</MERS>\n",
        "       <MERS>tag2</MERS>\n",
        "    </Result>\n",
        "    :param filename: Name of the XML file used in the QA system\n",
        "    \"\"\"\n",
        "    origTree = ET.parse(filename)\n",
        "    root = origTree.getroot()\n",
        "\n",
        "    # Find the QP element and grab each query\n",
        "    Q = root.find(\"Q\")\n",
        "    question = Q.text\n",
        "    QP = Q.find(\"QP\")\n",
        "    query = QP.find(\"Query\").text\n",
        "    IR = Q.find(\"IR\")\n",
        "\n",
        "    # Use the query as the search term in the IR system (assumed indexed)\n",
        "\n",
        "    # pubmed_indexer = PubmedIndexer()\n",
        "    # pubmed_indexer.mk_index(overwrite=True)\n",
        "    # reader = PubmedReader()\n",
        "    # articles = reader.process_xml_frags('gdrive/My Drive/Colab Notebooks/BioASQ/data2', max_article_count=10000)\n",
        "    # pubmed_indexer.index_docs(articles, limit=10000)\n",
        "    results = pubmed_indexer.search(query)\n",
        "\n",
        "    # Create a subelement for each part of the result (there can be many)\n",
        "    for pa in results:\n",
        "        result = ET.SubElement(IR, \"Result\")\n",
        "        result.set(\"PMID\", pa.pmid)\n",
        "        journal = ET.SubElement(result, \"Journal\")\n",
        "        journal.text = pa.journal\n",
        "        year = ET.SubElement(result, \"Year\")\n",
        "        year.text = pa.year\n",
        "        title = ET.SubElement(result, \"Title\")\n",
        "        title.text = pa.title\n",
        "        abstract = ET.SubElement(result, \"Abstract\")\n",
        "        abstract.text = pa.abstract_text\n",
        "        for mesh in pa.mesh_major:\n",
        "            mesh_major = ET.SubElement(result, \"MeSH\")\n",
        "            mesh_major.text = mesh\n",
        "\n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write(filename, pretty_print=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtU54QF37Mhi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "9cd33483-8229-4180-9f47-526d31eea942"
      },
      "source": [
        "file = 'gdrive/My Drive/Colab Notebooks/BioASQ/qp_demo.xml'\n",
        "pubmed_indexer = PubmedIndexer()\n",
        "pubmed_indexer.mk_index(overwrite=True)\n",
        "reader = PubmedReader()\n",
        "articles = reader.process_xml_frags('gdrive/My Drive/Colab Notebooks/BioASQ/data2', max_article_count=100000)\n",
        "pubmed_indexer.index_docs(articles, limit=100000)\n",
        "origTree = ET.parse(file)\n",
        "root = origTree.getroot()\n",
        "for questions in root.findall('Q'):\n",
        "  extract_and_write(file)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "index object created\n",
            "adding documents\n",
            "fname gdrive/My Drive/Colab Notebooks/BioASQ/data2/pubmed20n1018.xml.gz articles 24328\n",
            "fname gdrive/My Drive/Colab Notebooks/BioASQ/data2/pubmed20n1052.xml.gz articles 1029\n",
            "fname gdrive/My Drive/Colab Notebooks/BioASQ/data2/pubmed20n1017.xml.gz articles 29999\n",
            "fname gdrive/My Drive/Colab Notebooks/BioASQ/data2/pubmed20n1042.xml.gz articles 159\n",
            "fname gdrive/My Drive/Colab Notebooks/BioASQ/data2/pubmed20n1016.xml.gz articles 15851\n",
            "fname gdrive/My Drive/Colab Notebooks/BioASQ/data2/pubmed20n1023.xml.gz articles 749\n",
            "fname gdrive/My Drive/Colab Notebooks/BioASQ/data2/pubmed20n1026.xml.gz articles 14962\n",
            "fname gdrive/My Drive/Colab Notebooks/BioASQ/data2/pubmed20n1025.xml.gz articles 5958\n",
            "reached max article count ending read\n",
            "fname gdrive/My Drive/Colab Notebooks/BioASQ/data2/pubmed20n1024.xml.gz articles 6965\n",
            "commiting index, added 100000 documents\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e4ea967bb523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morigTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mextract_and_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-8927d2298ac9>\u001b[0m in \u001b[0;36mextract_and_write\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}